{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[0725]05-3. Logistic Regression.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Application of Logistic Regression Classifier\n","\n","* $0 \\leq h_{Θ}(x)\\leq 1 $"],"metadata":{"id":"F4v-4Vwhli3p"}},{"cell_type":"markdown","source":["## Logistic Regression with Iris dataset"],"metadata":{"id":"v1NZSsaLGJGj"}},{"cell_type":"markdown","source":["### Setup"],"metadata":{"id":"1_HQNASNXuNt"}},{"cell_type":"code","source":["# common lib\n","import sklearn\n","import numpy as np"],"metadata":{"id":"-guJ4B_vYAxV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Datasets\n","\n"],"metadata":{"id":"mIl1CpUWbike"}},{"cell_type":"markdown","source":["#### Iris dataset\n","* feature 3 (petal width)\n","* label: (1 if Iris virginica, else 0)"],"metadata":{"id":"bID01cYLr_-Z"}},{"cell_type":"code","source":["from sklearn import datasets\n","import pandas as pd\n","\n","iris = datasets.load_iris()\n","iris_X = iris[\"data\"][:, 3:]  # petal width\n","iris_y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris virginica, else 0\n","\n","pd.DataFrame(iris_X).head(5)"],"metadata":{"id":"Ch2SQsdGeG0X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Preprocess"],"metadata":{"id":"xIVGmIJPsQ4Q"}},{"cell_type":"markdown","source":["#### train_test_split"],"metadata":{"id":"d-p4ppxzriZs"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","iris_X_train, iris_X_test, iris_y_train, iris_y_test = train_test_split(iris_X, iris_y, random_state=42)"],"metadata":{"id":"oz8J7Y8ZrjFR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Add out liers"],"metadata":{"id":"sYwqhJQ7zpun"}},{"cell_type":"code","source":["iris_X_train_out=np.append(iris_X_train, np.array([10., 10.,-20, -30]))\n","iris_y_train_out=np.append(iris_y_train, np.array([1,1,0, 0]))\n","iris_X_train_out=iris_X_train_out.reshape(-1,1)"],"metadata":{"id":"mSsLpnbnzxiE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Linear Regression"],"metadata":{"id":"EaMTznytntL7"}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","\n","line_model = LinearRegression()\n","line_model.fit(iris_X_train_out, iris_y_train_out)\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","plt.plot(iris_X_train_out, iris_y_train_out, 'o')\n","plt.plot(iris_X_train_out,line_model.predict(iris_X_train_out.reshape(-1,1)), \"k-\",markersize=1, linewidth=2)\n","\n","#Decision boundary\n","y = np.mean(line_model.predict(iris_X_train_out.reshape(-1,1)))\n","print(y)\n","plt.plot([y, y], [-1.1, 1.1], \"r-\")\n","plt.show()"],"metadata":{"id":"GJMbzZyonsM8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sigmoid"],"metadata":{"id":"K0rfg0RN3Rbn"}},{"cell_type":"code","source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","plt.plot(iris_X_train_out, iris_y_train_out, 'o')\n","\n","#sigmoid\n","t = np.linspace(-30, 30, 100)\n","b = 1.5\n","sig = 1 / (1 + np.exp(-t+b))\n","plt.plot(t, sig, \"k-\",markersize=1, linewidth=2)\n","\n","#Decision boundary\n","plt.plot([b, b], [-0.1, 1.1], \"r-\")\n","plt.show()"],"metadata":{"id":"ZxaivtMGxbQC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Logistic Regression Classifier"],"metadata":{"id":"rxREqpmxrDOR"}},{"cell_type":"markdown","source":["**- sklearn.linear_model.[LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) : Returns the instance itself.**\n"],"metadata":{"id":"i5ONse2UTp5h"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","log_reg_model = LogisticRegression(random_state=42)\n","log_reg_model.fit(iris_X_train_out, iris_y_train_out)"],"metadata":{"id":"bQyGua2vrGJC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Evaluation"],"metadata":{"id":"Gvlsef0KVpNt"}},{"cell_type":"code","source":["from sklearn import metrics\n","\n","predict = log_reg_model.predict(iris_X_train_out)\n","acc = metrics.accuracy_score(iris_y_train_out, predict)\n","print('Train acc(iris): {}'.format(acc))\n","\n","\n","predict = log_reg_model.predict(iris_X_test)\n","acc = metrics.accuracy_score(iris_y_test, predict)\n","print('Test acc(iris): {}'.format(acc))\n"],"metadata":{"id":"N1kwnbD-Vuvt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Logistic Regression with Breast cancer dataset"],"metadata":{"id":"SXf9OcDwGaq4"}},{"cell_type":"markdown","source":["### Setup"],"metadata":{"id":"5l6kUnCrGaq4"}},{"cell_type":"code","source":["# common lib\n","import sklearn\n","import numpy as np"],"metadata":{"id":"Bwna7H2WGaq4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Datasets\n","\n"],"metadata":{"id":"IMpt3-U6Gaq5"}},{"cell_type":"markdown","source":["#### Breast cancer dataset\n","* The breast cancer dataset is a classic and very easy binary classification dataset."],"metadata":{"id":"P_d7SuupGaq6"}},{"cell_type":"code","source":["from sklearn import datasets\n","import pandas as pd\n","\n","breast = datasets.load_breast_cancer()\n","breast_X = breast[\"data\"]\n","breast_y = breast[\"target\"]\n","breast_feature_name = breast.feature_names\n","\n","pd.DataFrame(breast_X, columns=breast_feature_name).head(5)"],"metadata":{"id":"gjdydeq7Gaq6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Preprocess"],"metadata":{"id":"196wJ4ZGGaq6"}},{"cell_type":"markdown","source":["#### Variance based feature selection"],"metadata":{"id":"CyJHjXglGaq6"}},{"cell_type":"code","source":["from sklearn.feature_selection import VarianceThreshold\n","from sklearn.preprocessing import MinMaxScaler\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","selector = VarianceThreshold().fit(MinMaxScaler().fit_transform(breast_X))\n","variances = selector.variances_\n","var_sort = np.argsort(variances)[::-1]\n","\n","plt.figure(figsize=(6, 2))\n","ypos = np.arange(5)[::-1]\n","plt.barh(ypos, variances[var_sort][:5], align='center')\n","plt.yticks(ypos, np.array(breast_feature_name)[var_sort][:5])\n","plt.xlabel(\"Variance\");\n","\n","index = np.where(breast_feature_name=='worst concave points')[0][0]\n","breast_X_texture = breast[\"data\"][:,index].reshape(-1,1)"],"metadata":{"id":"5RKNH9fqGaq7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["breast_X_high_var =VarianceThreshold(0.04).fit_transform(MinMaxScaler().fit_transform(breast_X))"],"metadata":{"id":"0HBH0tWhp5sg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### train_test_split"],"metadata":{"id":"J7az7i05Gaq7"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","breast_X_train, breast_X_test, breast_y_train, breast_y_test = train_test_split(breast_X_high_var, breast_y, random_state=42)"],"metadata":{"id":"MHqhX52aGaq7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Logistic Regression model"],"metadata":{"id":"h64UCZGJGaq9"}},{"cell_type":"markdown","source":["**- sklearn.linear_model.[LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) : Returns the instance itself.**\n"],"metadata":{"id":"Sg-K2BbjGaq9"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","log_reg_model_breast = LogisticRegression(random_state=42)\n","log_reg_model_breast.fit(breast_X_train, breast_y_train)"],"metadata":{"id":"A8I8ljEQGaq-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Evaluation"],"metadata":{"id":"jL5YECRcGaq-"}},{"cell_type":"code","source":["from sklearn import metrics\n","\n","predict = log_reg_model_breast.predict(breast_X_train)\n","acc = metrics.accuracy_score(breast_y_train, predict)\n","print('Train acc(breast): {}'.format(acc))\n","\n","\n","predict = log_reg_model_breast.predict(breast_X_test)\n","acc = metrics.accuracy_score(breast_y_test, predict)\n","print('Test acc(breast): {}'.format(acc))\n"],"metadata":{"id":"h1dEfgPSGaq_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Set parameter(C, max_iter, solver)\n","\n","**C**\n","  * float, default=1.0\n","  * Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n","  * 너무 작으면 강한 정규화로 underfitting 가능성이 높아짐\n","  * 너무 크면 outlier가 발생하여 overfitting 가능성이 높아짐\n","\n","**max_iter**\n","  * int, default=100\n","  * Maximum number of iterations taken for the solvers to converge.\n","\n","**solver**\n","\n","* default='lbfgs'\n","* solver 종류\n","  * newton-cg: ['l2', 'none']\n","  * lbfgs: ['l2', 'none']\n","  * liblinear: ['l1', 'l2']\n","  * sag: ['l2', 'none']\n","  * saga: ['elasticnet', 'l1', 'l2', 'none']\n","* solver 장점 및 용도\n","  * For small datasets, 'liblinear' is a good choice, whereas 'sag' and 'saga'are faster for large ones;\n","  * For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs' handle multinomial loss; 'liblinear' is limited to one-versus-rest schemes."],"metadata":{"id":"A2rHBHIPrVQ5"}},{"cell_type":"markdown","source":["* C 1e-3 VS 1e+3"],"metadata":{"id":"gPObH7k6rh0t"}},{"cell_type":"code","source":["# Default model\n","from sklearn.linear_model import LogisticRegression\n","log_reg_model_c = LogisticRegression(random_state=42)\n","\n","# C=10**-3\n","log_reg_model_c.set_params(C=10**-3)\n","log_reg_model_c.fit(breast_X_train, breast_y_train)\n","\n","predict = log_reg_model_c.predict(breast_X_train)\n","acc = metrics.accuracy_score(breast_y_train, predict)\n","print('train Accuracy(C=10**-3): {}'.format(acc))\n","predict = log_reg_model_c.predict(breast_X_test)\n","acc = metrics.accuracy_score(breast_y_test, predict)\n","print('Test Accuracy(C=10**-3): {}'.format(acc))\n","\n","# C=10**3\n","log_reg_model_c.set_params(C=10**3)\n","log_reg_model_c.fit(breast_X_train, breast_y_train)\n","\n","predict = log_reg_model_c.predict(breast_X_train)\n","acc = metrics.accuracy_score(breast_y_train, predict)\n","print('train Accuracy(C=10**3): {}'.format(acc))\n","predict = log_reg_model_c.predict(breast_X_test)\n","acc = metrics.accuracy_score(breast_y_test, predict)\n","print('Test Accuracy(C=10**3): {}'.format(acc))"],"metadata":{"id":"mavtWpB5rnmf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* max_iter 2 VS 100"],"metadata":{"id":"K6PZ-Bf1sgLm"}},{"cell_type":"code","source":["# Default model\n","from sklearn.linear_model import LogisticRegression\n","log_reg_model = LogisticRegression(random_state=42)\n","\n","# max_iter=2\n","log_reg_model.set_params(max_iter=2)\n","log_reg_model.fit(breast_X_train, breast_y_train)\n","\n","predict = log_reg_model.predict(breast_X_train)\n","acc = metrics.accuracy_score(breast_y_train, predict)\n","print('train Accuracy(max_iter=2): {}'.format(acc))\n","predict = log_reg_model.predict(breast_X_test)\n","acc = metrics.accuracy_score(breast_y_test, predict)\n","print('Test Accuracy(max_iter=2): {}'.format(acc))\n","\n","# max_iter=100\n","log_reg_model.set_params(max_iter=100)\n","log_reg_model.fit(breast_X_train, breast_y_train)\n","\n","predict = log_reg_model.predict(breast_X_train)\n","acc = metrics.accuracy_score(breast_y_train, predict)\n","print('train Accuracy(max_iter=100): {}'.format(acc))\n","predict = log_reg_model.predict(breast_X_test)\n","acc = metrics.accuracy_score(breast_y_test, predict)\n","print('Test Accuracy(max_iter=100): {}'.format(acc))"],"metadata":{"id":"jY4T1mHusrcO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* solver 'lbfgs' VS 'sag'"],"metadata":{"id":"DT4mBgpttXt6"}},{"cell_type":"code","source":["# Default model\n","from sklearn.linear_model import LogisticRegression\n","log_reg_model = LogisticRegression(max_iter=2, random_state=42)\n","\n","# solver=\"lbfgs\"\n","log_reg_model.set_params(solver=\"lbfgs\")\n","log_reg_model.fit(breast_X_train, breast_y_train)\n","\n","predict = log_reg_model.predict(breast_X_train)\n","acc = metrics.accuracy_score(breast_y_train, predict)\n","print('train Accuracy(solver=\"lbfgs\"): {}'.format(acc))\n","predict = log_reg_model.predict(breast_X_test)\n","acc = metrics.accuracy_score(breast_y_test, predict)\n","print('Test Accuracy(solver=\"lbfgs\"): {}'.format(acc))\n","\n","# solver=\"sag\"\n","log_reg_model.set_params(solver=\"sag\")\n","log_reg_model.fit(breast_X_train, breast_y_train)\n","\n","predict = log_reg_model.predict(breast_X_train)\n","acc = metrics.accuracy_score(breast_y_train, predict)\n","print('train Accuracy(solver=\"sag\"): {}'.format(acc))\n","predict = log_reg_model.predict(breast_X_test)\n","acc = metrics.accuracy_score(breast_y_test, predict)\n","print('Test Accuracy(solver=\"sag\"): {}'.format(acc))"],"metadata":{"id":"2rjbRBb7tgJw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Validation_curve\n"],"metadata":{"id":"Lxyh2mm4GarC"}},{"cell_type":"markdown","source":["* Visualization 함수"],"metadata":{"id":"ZgCtZ_s_GarC"}},{"cell_type":"code","source":["%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","\n","# 수치형 파라미터 시각화 함수\n","def viz_val_curve(param_range, train_mean, train_std, test_mean, test_std, param_name, xscale_log=False):\n","  plt.plot(param_range, train_mean, \n","          color='blue', marker='o', \n","          markersize=5, label='Training accuracy')\n","\n","  plt.fill_between(param_range, train_mean + train_std,\n","                  train_mean - train_std, alpha=0.15,\n","                  color='blue')\n","\n","  plt.plot(param_range, test_mean, \n","          color='green', linestyle='--', \n","          marker='s', markersize=5, \n","          label='Validation accuracy')\n","\n","  plt.fill_between(param_range, \n","                  test_mean + test_std,\n","                  test_mean - test_std, \n","                  alpha=0.15, color='green')\n","\n","\n","  plt.grid()\n","  plt.legend(loc='lower right')\n","  if xscale_log:\n","    plt.xscale('log')\n","  plt.xlabel(param_name)\n","  plt.ylabel('Accuracy')\n","  plt.ylim([np.min(test_mean)*0.8, np.max(train_mean)*1.2])\n","  plt.tight_layout()\n","  plt.show()\n","\n","# 범주형 파라미터 시각화 함수\n","def viz_val_bar(param_range, train_mean, train_std, test_mean, test_std, param_name):\n","  idx = np.arange(len(param_range))\n","  plt.bar(idx, test_mean, width=0.3)\n","  plt.xlabel(param_name)\n","  plt.ylabel('Accuracy')\n","  plt.ylim([np.min(test_mean)*0.9, np.max(test_mean)*1.1])\n","  plt.xticks(idx, param_range, fontsize=15)\n","  plt.show()"],"metadata":{"id":"zly3G8WRGarC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Validation_curve(C)\n","\n","#### **C**\n","  * float, default=1.0\n","  * Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n","  * 너무 작으면 강한 정규화로 underfitting 가능성이 높아짐\n","  * 너무 크면 outlier가 발생하여 overfitting 가능성이 높아짐"],"metadata":{"id":"MSxyzma0GarF"}},{"cell_type":"code","source":["from sklearn.model_selection import validation_curve\n","\n","param_range= [10**i for i in range(-9,-3)]\n","param_name='C'\n","\n","from sklearn.linear_model import LogisticRegression\n","log_reg_model_c = LogisticRegression(random_state=42, solver='liblinear')\n","\n","train_scores, test_scores = validation_curve(\n","                estimator=log_reg_model_c, \n","                X=breast_X, \n","                y=breast_y, \n","                param_name=param_name, \n","                param_range=param_range,\n","                cv=10)\n","\n","train_mean = np.mean(train_scores, axis=1)\n","train_std = np.std(train_scores, axis=1)\n","test_mean = np.mean(test_scores, axis=1)\n","test_std = np.std(test_scores, axis=1)\n","\n","viz_val_curve(param_range, train_mean, train_std, test_mean, test_std, param_name, xscale_log=True)"],"metadata":{"id":"FrCOVj4pGarG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Evaluation\n","**Default model performance**\n","* Train acc(breast): 0.7723004694835681\n","* Test acc(breast): 0.8251748251748252"],"metadata":{"id":"Uckzl93fGarG"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","proper_model_c = LogisticRegression(C=10**2, random_state=42)\n","proper_model_c.fit(breast_X_train, breast_y_train)\n","\n","from sklearn import metrics\n","\n","predict = proper_model_c.predict(breast_X_train)\n","acc = metrics.accuracy_score(breast_y_train, predict)\n","print('Train Accuracy(c): {}'.format(acc))\n","\n","predict = proper_model_c.predict(breast_X_test)\n","acc = metrics.accuracy_score(breast_y_test, predict)\n","print('Test Accuracy(c): {}'.format(acc))"],"metadata":{"id":"gHD3TnudGarG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise"],"metadata":{"id":"H6NEj_ezY3-W"}},{"cell_type":"markdown","source":["## 1번 문제\n","Logistic Regression을 사용하여 Wine 데이터를 분류 하시오.\n","\n","  * Validation_curve 함수를 사용하여 아래 Hyperparameters의 변화에 따른 결과를 그래프로 표현하시오.\n","    * C\n","    * max_iter\n","    * solver\n","  * 가장 높은 accuracy를 기록하는 파리미터 조합을 도출하시오.\n","\n","  ```python\n","  from sklearn.datasets import load_wine\n","\n","  wine = load_wine()\n","  ```"],"metadata":{"id":"RID-hzOgY9IJ"}},{"cell_type":"markdown","source":["## 1번 문제 답안"],"metadata":{"id":"-_C6I1m3Ky3t"}},{"cell_type":"code","source":[""],"metadata":{"id":"7Nyys5PDSfZk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2번 문제\n","Logistic Regression을 사용하여 California Housing 데이터를 분류 하시오.\n","\n","  * Validation_curve 함수를 사용하여 아래 Hyperparameters의 변화에 따른 결과를 그래프로 표현하시오.\n","    * C\n","    * max_iter\n","    * solver\n","  * 가장 높은 accuracy를 기록하는 파리미터 조합을 도출하시오.\n","\n","  \n","\n","#### California Housing dataset\n","\n","* The target variable is the median house value for California districts, expressed in hundreds of thousands of dollars ($100,000).\n","*  하였음\n","\n","```python\n","#load data\n","housing = fetch_california_housing()\n","housing_X = housing.data\n","housing_y = np.round(housing.target).astype(int) # make y discrete\n","print('Number of target: ',len(set(housing_y)))\n","\n","pd.DataFrame(housing_X, columns=housing.feature_names).head(3)\n","```\n","\n"],"metadata":{"id":"AP8NYPKcKGnp"}},{"cell_type":"markdown","source":["## 2번 문제 답안"],"metadata":{"id":"LZVxSUI6K3GE"}},{"cell_type":"code","source":[""],"metadata":{"id":"5wvjredzSeuH"},"execution_count":null,"outputs":[]}]}