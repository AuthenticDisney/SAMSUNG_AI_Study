{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5-2 답안.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Exercise"],"metadata":{"id":"TLwJPm52uGlX"}},{"cell_type":"markdown","source":["## 1번 문제 (Gaussian Naive Bayes)\n","\n","Gausian Naive Bayes model을 사용하여 fetch_california_housing 데이터를 분류 하시오.\n","  * Validation_curve 함수를 사용하여 아래 Hyperparameters의 변화에 따른 결과를 그래프로 표현하시오.\n","    * var_smoothing\n","  * 가장 높은 accuracy를 기록하는 파리미터를 도출하시오.\n","\n","```python\n","from sklearn.datasets import fetch_california_housing\n","import pandas as pd\n","\n","housing = fetch_california_housing()\n","housing_X = housing.data\n","\n","# discrete한 y 데이터로 변경하기 위해 소수점 자리를 반올림하여 int로 형변환\n","housing_y = np.round(housing.target).astype(int) # make y discrete\n","print('Number of target: ',len(set(housing_y)))\n","\n","pd.DataFrame(housing_X, columns=housing.feature_names).head(3)\n","\n","```"],"metadata":{"id":"W0MAZvI5FeXD"}},{"cell_type":"markdown","source":["## 1번 문제 답안"],"metadata":{"id":"IuWC5oLMQTIo"}},{"cell_type":"markdown","source":["### Setup"],"metadata":{"id":"rySoWG16QI0p"}},{"cell_type":"code","source":["# Common imports\n","import sklearn\n","import numpy as np"],"metadata":{"id":"iNxL-NNsQI0p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load data\n","\n","#### California Housing dataset\n","\n","* The target variable is the median house value for California districts, expressed in hundreds of thousands of dollars ($100,000)."],"metadata":{"id":"8DEmuG4jQI0p"}},{"cell_type":"code","source":["from sklearn.datasets import fetch_california_housing\n","import pandas as pd\n","\n","housing = fetch_california_housing()\n","housing_X = housing.data\n","housing_y = np.round(housing.target).astype(int) # make y discrete\n","print('Number of target: ',len(set(housing_y)))\n","\n","pd.DataFrame(housing_X, columns=housing.feature_names).head(3)"],"metadata":{"id":"LgrPLxlbQI0p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Splitting"],"metadata":{"id":"jtXIl3UPQI0q"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","housing_X_train, housing_X_test, housing_y_train, housing_y_test = train_test_split(housing_X, housing_y, random_state=42)"],"metadata":{"id":"kYK8n29zQI0q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GaussianNB model"],"metadata":{"id":"kD69jF9JQI0q"}},{"cell_type":"markdown","source":["**- sklearn.naive_bayes.[GaussianNB(*, priors=None, var_smoothing=1e-09)](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) : Returns the instance itself.**\n","\n","Can perform online updates to model parameters via partial_fit."],"metadata":{"id":"3VB534fnQI0q"}},{"cell_type":"code","source":["from sklearn.naive_bayes import GaussianNB\n","\n","model = GaussianNB()\n","model.fit(housing_X_train, housing_y_train)"],"metadata":{"id":"jOn8h8doQI0q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Evaluation"],"metadata":{"id":"zBTmarjhRZyP"}},{"cell_type":"code","source":["from sklearn import metrics\n","\n","predict = model.predict(housing_X_train)\n","acc = metrics.accuracy_score(housing_y_train, predict)\n","print('Train Accuracy: {}'.format(acc))\n","\n","\n","predict = model.predict(housing_X_test)\n","acc = metrics.accuracy_score(housing_y_test, predict)\n","print('Test Accuracy: {}'.format(acc))"],"metadata":{"id":"wc3DWUj1RaPH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Validation_curve(param_name='var_smoothing')"],"metadata":{"id":"K14rvUz_R3xt"}},{"cell_type":"code","source":["from sklearn.model_selection import validation_curve\n","\n","param_range= [10**i for i in range(-11,4)]\n","\n","from sklearn.naive_bayes import GaussianNB\n","\n","smooth_model = GaussianNB() # change this\n","smooth_model.fit(housing_X_train, housing_y_train)\n","\n","train_scores, test_scores = validation_curve(\n","                estimator=smooth_model, \n","                X=housing_X_train, \n","                y=housing_y_train, \n","                param_name='var_smoothing', \n","                param_range=param_range,\n","                cv=2)\n","\n","train_mean = np.mean(train_scores, axis=1)\n","train_std = np.std(train_scores, axis=1)\n","test_mean = np.mean(test_scores, axis=1)\n","test_std = np.std(test_scores, axis=1)"],"metadata":{"id":"-x-Yl9y5R3xt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Visualization"],"metadata":{"id":"3faTbp_gR3xt"}},{"cell_type":"code","source":["%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","\n","plt.plot(param_range, train_mean, \n","         color='blue', marker='o', \n","         markersize=5, label='Training accuracy')\n","\n","plt.fill_between(param_range, train_mean + train_std,\n","                 train_mean - train_std, alpha=0.15,\n","                 color='blue')\n","\n","plt.plot(param_range, test_mean, \n","         color='green', linestyle='--', \n","         marker='s', markersize=5, \n","         label='Validation accuracy')\n","\n","plt.fill_between(param_range, \n","                 test_mean + test_std,\n","                 test_mean - test_std, \n","                 alpha=0.15, color='green')\n","\n","\n","plt.grid()\n","plt.xscale('log')\n","plt.legend(loc='lower right')\n","plt.xlabel('var_smoothing')\n","plt.ylabel('Accuracy')\n","plt.ylim([np.min(train_mean)*0.8, np.max(train_mean)*1.2])\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"QURSQHkPR3xt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Evaluation\n","**Default model performance**\n","* Train Accuracy: 0.2554909560723514\n","* Test Accuracy: 0.27015503875968994"],"metadata":{"id":"xHllNfFH92Rc"}},{"cell_type":"code","source":["from sklearn.naive_bayes import GaussianNB\n","\n","proper_model = GaussianNB(var_smoothing=10**-6)\n","proper_model.fit(housing_X_train, housing_y_train)\n","\n","from sklearn import metrics\n","\n","predict = proper_model.predict(housing_X_train)\n","acc = metrics.accuracy_score(housing_y_train, predict)\n","print('Train Accuracy: {}'.format(acc))\n","\n","\n","predict = proper_model.predict(housing_X_test)\n","acc = metrics.accuracy_score(housing_y_test, predict)\n","print('Test Accuracy: {}'.format(acc))"],"metadata":{"id":"vBVoLHQq92Rc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Result\n","**Best performance**\n","* Train Accuracy: 0.3303617571059432\n","* Test Accuracy: 0.3412790697674419"],"metadata":{"id":"yJTTZOTx-59L"}},{"cell_type":"markdown","source":["## 2번 문제 (Bernoulli Naive Bayes)\n","Bernoulli Naive Bayes model을 사용하여 Forest CoverType index 10~53까지의 데이터를 분류 하시오.\n","  * Validation_curve 함수를 사용하여 아래 Hyperparameters의 변화에 따른 결과를 그래프로 표현하시오.\n","    * alpha\n","  * 가장 높은 accuracy를 기록하는 파리미터를 도출하시오.\n","\n","```python\n","from sklearn.datasets import fetch_covtype\n","import pandas as pd\n","\n","covtype = fetch_covtype()\n","# covtype index 10~53\n","bi_covtype_X = covtype.data[:,10:53] #discrete features for Bernoulli model\n","bi_covtype_y = covtype.target\n","\n","covtype_feature_name = covtype.feature_names[10:53]\n","print('Number of targets: ',len(set(bi_covtype_y)))\n","\n","pd.DataFrame(bi_covtype_X, columns=covtype_feature_name).head(3)\n","```\n","\n"],"metadata":{"id":"RkirLKxZjDRk"}},{"cell_type":"markdown","source":["## 2번 문제 답안"],"metadata":{"id":"0A6RiPLfGRlT"}},{"cell_type":"markdown","source":["### Setup"],"metadata":{"id":"u3pUV5OxGnbb"}},{"cell_type":"code","source":["# Common imports\n","import sklearn\n","import numpy as np"],"metadata":{"id":"ZKpU-0SmGnbf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load data\n","\n","#### Forest CoverType dataset\n","* Characteristic data of forest covertype\n","* Predict which type of covertype belongs to\n","* https://archive.ics.uci.edu/ml/datasets/Covertype \n","* $Y$: discrete, \n","  * $X_{0 ∼ 9}$: continuous\n","  * $X_{10 ∼ 53}$: binary"],"metadata":{"id":"5ueU2QEcGnbf"}},{"cell_type":"code","source":["from sklearn.datasets import fetch_covtype\n","import pandas as pd\n","\n","covtype = fetch_covtype()\n","# covtype index 10~53\n","bi_covtype_X = covtype.data[:,10:53] #discrete features for Bernoulli model\n","bi_covtype_y = covtype.target\n","\n","covtype_feature_name = covtype.feature_names[10:53]\n","print('Number of targets: ',len(set(bi_covtype_y)))\n","\n","pd.DataFrame(bi_covtype_X, columns=covtype_feature_name).head(3)"],"metadata":{"id":"XxrKe9XaGnbf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Splitting"],"metadata":{"id":"YtHYY_brGnbf"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","bi_covtype_X_train, bi_covtype_X_test, bi_covtype_y_train, bi_covtype_y_test = train_test_split(bi_covtype_X, bi_covtype_y, random_state=42)"],"metadata":{"id":"6z1I0x1KGnbf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### BernoulliNB model"],"metadata":{"id":"ILUgboUpSwMR"}},{"cell_type":"code","source":["from sklearn.naive_bayes import BernoulliNB\n","\n","bernoulli_model_cov = BernoulliNB()\n","bernoulli_model_cov.fit(bi_covtype_X_train, bi_covtype_y_train)"],"metadata":{"id":"rWzZKOpQTDrX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Evaluation"],"metadata":{"id":"aDypDl-LSwMS"}},{"cell_type":"code","source":["from sklearn import metrics\n","\n","# use covtype binary data\n","predict = bernoulli_model_cov.predict(bi_covtype_X_train)\n","acc = metrics.accuracy_score(bi_covtype_y_train, predict)\n","print('Train Accuracy(covtype): {}'.format(acc))\n","predict = bernoulli_model_cov.predict(bi_covtype_X_test)\n","acc = metrics.accuracy_score(bi_covtype_y_test, predict)\n","print('Test Accuracy(covtype): {}'.format(acc))"],"metadata":{"id":"Fhaj1tVpSwMS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Validation_curve(alpha)"],"metadata":{"id":"cJYmmxgvIY-p"}},{"cell_type":"code","source":["from sklearn.model_selection import validation_curve\n","\n","param_range= [10**i for i in range(-11,4)]\n","\n","from sklearn.naive_bayes import BernoulliNB\n","\n","smooth_model = BernoulliNB()\n","smooth_model.fit(bi_covtype_X_train, bi_covtype_y_train)\n","\n","train_scores, test_scores = validation_curve(\n","                estimator=smooth_model, \n","                X=bi_covtype_X_train, \n","                y=bi_covtype_y_train, \n","                param_name='alpha', \n","                param_range=param_range,\n","                cv=2)\n","\n","train_mean = np.mean(train_scores, axis=1)\n","train_std = np.std(train_scores, axis=1)\n","test_mean = np.mean(test_scores, axis=1)\n","test_std = np.std(test_scores, axis=1)"],"metadata":{"id":"CzOf5rrfIY-u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Visualization"],"metadata":{"id":"zGxWV634IY-u"}},{"cell_type":"code","source":["%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","\n","plt.plot(param_range, train_mean, \n","         color='blue', marker='o', \n","         markersize=5, label='Training accuracy')\n","\n","plt.fill_between(param_range, train_mean + train_std,\n","                 train_mean - train_std, alpha=0.15,\n","                 color='blue')\n","\n","plt.plot(param_range, test_mean, \n","         color='green', linestyle='--', \n","         marker='s', markersize=5, \n","         label='Validation accuracy')\n","\n","plt.fill_between(param_range, \n","                 test_mean + test_std,\n","                 test_mean - test_std, \n","                 alpha=0.15, color='green')\n","\n","\n","plt.grid()\n","plt.xscale('log')\n","plt.legend(loc='lower right')\n","plt.xlabel('alpha')\n","plt.ylabel('Accuracy')\n","plt.ylim([np.min(train_mean)*0.8, np.max(train_mean)*1.2])\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"f-xS6ywKIY-u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Evaluation\n","**Default model performance**\n","* Train Accuracy(covtype): 0.6237874604999553\n","* Test Accuracy(covtype): 0.6204071516595182"],"metadata":{"id":"ohcrueiWIY-u"}},{"cell_type":"code","source":["from sklearn.naive_bayes import BernoulliNB\n","\n","proper_model_tfidf = BernoulliNB(alpha=10**2)\n","proper_model_tfidf.fit(bi_covtype_X_train, bi_covtype_y_train)\n","\n","from sklearn import metrics\n","\n","# use covtype binary data\n","predict = proper_model_tfidf.predict(bi_covtype_X_train)\n","acc = metrics.accuracy_score(bi_covtype_y_train, predict)\n","print('Train Accuracy(cov): {}'.format(acc))\n","predict = proper_model_tfidf.predict(bi_covtype_X_test)\n","acc = metrics.accuracy_score(bi_covtype_y_test, predict)\n","print('Test Accuracy(cov): {}'.format(acc))"],"metadata":{"id":"UuBivn2hIY-u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Result\n","**Best performance**\n","* Train Accuracy(cov): 0.6338297086233445\n","* Test Accuracy(cov): 0.6310919567926307"],"metadata":{"id":"YajgGZFD_myO"}},{"cell_type":"markdown","source":["## 3번 문제(Multinomial Naive Bayes) \n","\n","Multinomial Naive Bayes model을 사용하여 20 Newsgroup 데이터를 분류 하시오.\n","  * Hashing Vectoizer를 사용하여 text features를 vectorize 하시오.\n","    * Hashing Vectoizer의 파라미터 binary에 True값을 할당하여 데이터를 이진 데이터로 벡터화 하시오.\n","\n","  ```python\n","  from sklearn.feature_extraction.text import HashingVectorizer\n","\n","  # Hashing Vectoizer\n","  hash_vectorizer = HashingVectorizer(n_features=1000, binary=True)\n","  ```\n","  * Validation_curve 함수를 사용하여 아래 Hyperparameters의 변화에 따른 결과를 그래프로 표현하시오.\n","    * alpha\n","  * 가장 높은 accuracy를 기록하는 파리미터를 도출하시오."],"metadata":{"id":"bC-8W0ySuGZP"}},{"cell_type":"markdown","source":["## 3번 문제 답안"],"metadata":{"id":"3Lxoygg7UdMg"}},{"cell_type":"markdown","source":["### Setup"],"metadata":{"id":"7mGbTRQPLXQn"}},{"cell_type":"code","source":["# Common imports\n","import sklearn\n","import numpy as np"],"metadata":{"id":"rGg5BuTPLXQv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load data\n","\n","####20 Newsgroup dataset\n","* Categorize which group the news article belongs to\n","\n","* News articles are text data, so special preprocessing is required."],"metadata":{"id":"0rgt2qohLXQv"}},{"cell_type":"code","source":["from sklearn.datasets import fetch_20newsgroups\n","import pandas as pd\n","\n","newsgroup = fetch_20newsgroups()\n","\n","newsgroup_X = newsgroup.data\n","newsgroup_y = newsgroup.target\n","\n","pd.DataFrame(newsgroup.data).head(3)"],"metadata":{"id":"Ss7GO5JOLXQv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Splitting"],"metadata":{"id":"UE2-1tgdLXQv"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","newsgroup_X_train, newsgroup_X_test, newsgroup_y_train, newsgroup_y_test = train_test_split(newsgroup_X, newsgroup_y, random_state=42)"],"metadata":{"id":"tI1V5toKLXQv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Vectorization for text data\n","* sklearn.feature_extraction.text.[CountVectorizer()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n","  * 문서를 토큰 리스트로 변환\n","  * 각 문서에서 토큰의 출현 빈도 카운트\n","  * 각 문서를 Bag of words(BOW) 인코딩 벡터로 변환\n","* sklearn.feature_extraction.text.[HashingVectorizer()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer)\n","  * 해시 함수 사용으로 단어에 대한 index를 생성하여 실행시간 단축\n","* sklearn.feature_extraction.text.[TfidfVectorizer()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n","  * 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 가중치를 축소\n","  * $tf-idf(d, t)= tf(d, f)̇ ⋅ idf(t)$\n","    * $d, t$ : document, term\n","    * $tf(d,t)$ : term frequency.\n","    * $idf(t)$ : inverse document frequency.\n","    * $idf(d, t)= log\\frac{n}{1+df(t)}$\n","      * $n$ : 전체 문서의 수\n","      * $df$ : 단어 $t$를 가진 문서의 수\n"],"metadata":{"id":"_v0eC-JPLXQw"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import HashingVectorizer\n","\n","# Hashing Vectoizer\n","hash_vectorizer = HashingVectorizer(n_features=1000, binary=True)\n","X_train_hash = hash_vectorizer.fit_transform(newsgroup_X_train)\n","X_test_hash = hash_vectorizer.transform(newsgroup_X_test)"],"metadata":{"id":"XCLeOxSnLXQw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### MultinomialNB model"],"metadata":{"id":"4kTL7r6LLXQw"}},{"cell_type":"markdown","source":["**- sklearn.naive_bayes.[MultinomialNB(*, alpha=1.0, fit_prior=True, class_prior=None)](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) : Returns the instance itself.**\n","\n","The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification)."],"metadata":{"id":"2CJ7FV2GLXQw"}},{"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB\n","\n","model_hash = MultinomialNB()\n","model_hash.fit(X_train_hash, newsgroup_y_train)"],"metadata":{"id":"50jcJT_DLXQw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Evaluation"],"metadata":{"id":"-Pk5Y8WdLXQw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TI16Z7sLXQw"},"outputs":[],"source":["from sklearn import metrics\n","\n","# use Hash Vector\n","predict = model_hash.predict(X_train_hash)\n","acc = metrics.accuracy_score(newsgroup_y_train, predict)\n","print('Train Accuracy(hash): {}'.format(acc))\n","predict = model_hash.predict(X_test_hash)\n","acc = metrics.accuracy_score(newsgroup_y_test, predict)\n","print('Test Accuracy(hash): {}'.format(acc))"]},{"cell_type":"markdown","source":["### Validation_curve(alpha)"],"metadata":{"id":"UMEQRBEEKr1t"}},{"cell_type":"code","source":["from sklearn.model_selection import validation_curve\n","param_range= [10**i for i in range(-11,4)]\n","\n","from sklearn.naive_bayes import MultinomialNB\n","\n","smooth_model = MultinomialNB()\n","smooth_model.fit(X_train_hash, newsgroup_y_train)\n","\n","train_scores, test_scores = validation_curve(\n","                estimator=smooth_model, \n","                X=X_train_hash, \n","                y=newsgroup_y_train, \n","                param_name='alpha', \n","                param_range=param_range,\n","                cv=2)\n","\n","train_mean = np.mean(train_scores, axis=1)\n","train_std = np.std(train_scores, axis=1)\n","test_mean = np.mean(test_scores, axis=1)\n","test_std = np.std(test_scores, axis=1)"],"metadata":{"id":"tEjTYwm0Kr1x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Visualization"],"metadata":{"id":"unphqXPHKr1x"}},{"cell_type":"code","source":["%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","\n","plt.plot(param_range, train_mean, \n","         color='blue', marker='o', \n","         markersize=5, label='Training accuracy')\n","\n","plt.fill_between(param_range, train_mean + train_std,\n","                 train_mean - train_std, alpha=0.15,\n","                 color='blue')\n","\n","plt.plot(param_range, test_mean, \n","         color='green', linestyle='--', \n","         marker='s', markersize=5, \n","         label='Validation accuracy')\n","\n","plt.fill_between(param_range, \n","                 test_mean + test_std,\n","                 test_mean - test_std, \n","                 alpha=0.15, color='green')\n","\n","\n","plt.grid()\n","plt.xscale('log')\n","plt.legend(loc='lower right')\n","plt.xlabel('alpha')\n","plt.ylabel('Accuracy')\n","plt.ylim([np.min(train_mean)*0.8, np.max(train_mean)*1.2])\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"bkr5H-x6Kr1x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Evaluation\n","**Deafault model performance**\n","* Train Accuracy(hash): 0.7721862109605185\n","* Test Accuracy(hash): 0.68045245669848"],"metadata":{"id":"_d6um2OTKr1x"}},{"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB\n","\n","proper_model_tfidf = MultinomialNB(alpha=10**-1)\n","proper_model_tfidf.fit(X_train_hash, newsgroup_y_train)\n","\n","\n","from sklearn import metrics\n","\n","# use hash Vector\n","predict = proper_model_tfidf.predict(X_train_hash)\n","acc = metrics.accuracy_score(newsgroup_y_train, predict)\n","print('Train Accuracy(hash): {}'.format(acc))\n","predict = proper_model_tfidf.predict(X_test_hash)\n","acc = metrics.accuracy_score(newsgroup_y_test, predict)\n","print('Test Accuracy(hash): {}'.format(acc))"],"metadata":{"id":"8osjKfPSKr1x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Result\n","**Best performance**\n","* Train Accuracy(hash): 0.7959929286977018\n","* Test Accuracy(hash): 0.6938847649346058"],"metadata":{"id":"qGIAXzwl_rzZ"}}]}