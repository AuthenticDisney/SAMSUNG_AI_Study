{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[0725]06. 모델 성능 평가.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMobwSWNltVR956WHdU5cbu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Model Evaluation"],"metadata":{"id":"bJvf7gxFyzo7"}},{"cell_type":"markdown","source":["## Basics"],"metadata":{"id":"mp45rVDey2rf"}},{"cell_type":"markdown","source":["### Overfitting and Underfitting"],"metadata":{"id":"bqMmFe9uy7mr"}},{"cell_type":"markdown","source":["#### Over/Underfitting in LinearRegression with virtual data"],"metadata":{"id":"uNT_vsxbSlO5"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import cross_val_score\n","\n","# Ground Truth\n","def true_fun(X):\n","    return np.cos(1.5 * np.pi * X)\n","\n","# Virtual data\n","np.random.seed(0)\n","n_samples = 30\n","degrees = [1, 4, 16]\n","\n","X = np.sort(np.random.rand(n_samples))\n","# small noise\n","y = true_fun(X) + np.random.randn(n_samples) * 0.1\n","\n","#visualization\n","plt.figure(figsize=(14, 5))\n","tradeoffs=['x\\nhigh-bias\\n low variance', 'x\\nmedian', 'x\\nlow-bias\\n high variance']\n","for i in range(len(degrees)):\n","    ax = plt.subplot(1, len(degrees), i + 1)\n","    plt.setp(ax, xticks=(), yticks=())\n","\n","    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)\n","    linear_regression = LinearRegression()\n","    pipeline = Pipeline(\n","        [\n","            (\"polynomial_features\", polynomial_features),\n","            (\"linear_regression\", linear_regression),\n","        ]\n","    )\n","    pipeline.fit(X[:, np.newaxis], y)\n","\n","    # Evaluate the models using crossvalidation\n","    scores = cross_val_score(\n","        pipeline, X[:, np.newaxis], y, scoring=\"neg_mean_squared_error\", cv=10\n","    )\n","\n","    X_test = np.linspace(0, 1, 100)\n","    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n","    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n","    plt.scatter(X, y, edgecolor=\"b\", s=20, label=\"Samples\")\n","    plt.xlabel(tradeoffs[i])\n","    plt.ylabel(\"y\")\n","    plt.xlim((0, 1))\n","    plt.ylim((-2, 2))\n","    plt.legend(loc=\"best\")\n","    plt.title(\n","        \"Degree {}\\nMSE = {:.2f}(+/- {:.2f})\".format(\n","            degrees[i], -scores.mean(), scores.std()\n","        )\n","    )\n","plt.show()"],"metadata":{"id":"gPCDE17C1kMM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Over/Underfitting in DecisionTreeClassifier with virtual data\n"],"metadata":{"id":"r_9gu6Wq3fKM"}},{"cell_type":"code","source":["from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.tree import DecisionTreeClassifier\n","from matplotlib import pyplot\n","\n","# create dataset\n","X, y = make_classification(n_samples=10000, n_features=20, n_informative=5, n_redundant=15, random_state=1)\n","\n","# split into train test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n","\n","# define lists to collect scores\n","train_scores, test_scores = list(), list()\n","\n","# define the tree depths to evaluate\n","values = [i for i in range(1, 21)]\n","\n","# evaluate a decision tree for each depth\n","for i in values:\n","\t# configure the model\n","\tmodel = DecisionTreeClassifier(max_depth=i)\n","\t# fit model on the training dataset\n","\tmodel.fit(X_train, y_train)\n","\t# evaluate on the train dataset\n","\ttrain_yhat = model.predict(X_train)\n","\ttrain_acc = accuracy_score(y_train, train_yhat)\n","\ttrain_scores.append(train_acc)\n","\t# evaluate on the test dataset\n","\ttest_yhat = model.predict(X_test)\n","\ttest_acc = accuracy_score(y_test, test_yhat)\n","\ttest_scores.append(test_acc)\n","\t# summarize progress\n","\tprint('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))\n","\n","# plot of train and test scores vs tree depth\n","pyplot.plot(values, train_scores, '-o', label='Train')\n","pyplot.plot(values, test_scores, '-o', label='Test')\n","pyplot.legend()\n","pyplot.show()"],"metadata":{"id":"ZDCJEWv84ksk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Bias and Variance"],"metadata":{"id":"xMn9U0SQy47S"}},{"cell_type":"markdown","source":["##### Setup"],"metadata":{"id":"uwgU-0Sf8-4T"}},{"cell_type":"code","source":["# common lib\n","import sklearn\n","import numpy as np"],"metadata":{"id":"x-NBMR0y8_rL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### California Housing dataset\n","\n","* The target variable is the median house value for California districts, expressed in hundreds of thousands of dollars.\n","* target값을 $100,000 기준으로 반올림하여 6개의 class로 범주화 함"],"metadata":{"id":"KLrS8fXLicDs"}},{"cell_type":"code","source":["from sklearn.datasets import fetch_california_housing\n","import pandas as pd\n","\n","housing = fetch_california_housing()\n","housing_X = housing.data\n","housing_y = np.round(housing.target).astype(int) # make y discrete\n","housing_feature_names = housing.feature_names\n","\n","print('Number of target: ',len(set(housing_y)))\n","\n","# combine features and class data\n","housing_np = np.append(housing_X, housing_y.reshape(-1,1), axis=1)\n","housing_col_names = housing_feature_names + ['class']\n","\n","housing_pd = pd.DataFrame(housing_np, columns=housing_col_names)\n","housing_pd.head(3)"],"metadata":{"id":"zSk4diO3icDs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### train_test_split"],"metadata":{"id":"bAPEyyi6bEy3"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","housing_X_train, housing_X_test, housing_y_train, housing_y_test = train_test_split(housing_X, housing_y, random_state=42)"],"metadata":{"id":"wAtYjS-7bEy3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Models Complexity\n","\n","* High Vias, Low Variance(Model Complexity is Low)\n","  * High k in kNN Classifier\n","  * Low max_depth in DecisionTree Classifier\n","* Low Vias, High Variance(Model Complexity is High)\n","  * Low k in kNN Classifier\n","  * High max_depth in DecisionTree Classifier"],"metadata":{"id":"K5IBCy3hRCgR"}},{"cell_type":"markdown","source":["* visualization 함수"],"metadata":{"id":"WbFGUNubFK_0"}},{"cell_type":"code","source":["%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","def viz_val_bar(param_range, test_acc, name):\n","  idx = np.arange(len(param_range))\n","  plt.figure(figsize=(10,5))\n","  colors = sns.color_palette('hls',len(param_range))\n","  bars = plt.bar(idx, test_acc, width=0.3, color=colors)\n","  plt.ylabel('Accuracy')\n","  plt.xlabel(name, fontsize=15, rotation=30)\n","  plt.legend(handles=bars, labels=param_range)\n","  plt.show()"],"metadata":{"id":"1D-4jC5JFJN0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Model Complexity**"],"metadata":{"id":"TsNRbP3RSkz1"}},{"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import BernoulliNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","\n","\n","clf1 = KNeighborsClassifier(n_neighbors=100)\n","clf2 = KNeighborsClassifier(n_neighbors=5)\n","clf3 = KNeighborsClassifier(n_neighbors=1)\n","\n","clf4 = DecisionTreeClassifier(max_depth=4, random_state=42)\n","clf5 = DecisionTreeClassifier(max_depth=8, random_state=42)\n","clf6 = DecisionTreeClassifier(max_depth=64, random_state=42)\n","\n","from sklearn.metrics import accuracy_score\n","\n","clfs = [clf1, clf2, clf3, clf4, clf5, clf6]\n","\n","raw_train_accs = []\n","raw_test_accs = []\n","class_names = []\n","for clf in clfs:\n","\n","  class_name = clf.__class__.__name__\n","  class_names.append(class_name)\n","\n","  clf.fit(housing_X_train, housing_y_train)\n","  \n","  pred = clf.predict(housing_X_train)\n","  raw_train_acc = accuracy_score(housing_y_train, pred)\n","  raw_train_accs.append(raw_train_acc)\n","  print('{0} Train 정확도: {1:.4f}'.format(class_name, raw_train_acc))\n","\n","  pred = clf.predict(housing_X_test)\n","  raw_test_acc = accuracy_score(housing_y_test, pred)\n","  raw_test_accs.append(raw_test_acc)\n","  print('{0} Test 정확도: {1:.4f}'.format(class_name, raw_test_acc))\n","\n","viz_val_bar(class_names, raw_train_accs, 'train')\n","viz_val_bar(class_names, raw_test_accs, 'test')\n"],"metadata":{"id":"SWJ3tH8jFSLg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Holdout method"],"metadata":{"id":"0tALkCoWy-4a"}},{"cell_type":"markdown","source":["##### Setup"],"metadata":{"id":"2Xt1nsixWhRg"}},{"cell_type":"code","source":["# common lib\n","import sklearn\n","import numpy as np"],"metadata":{"id":"1fvmpRwMWhRh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### train_test_split\n","\n","**- sklearn.model_selection.[train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)**\n","\n","Split arrays or matrices into random train and test subsets."],"metadata":{"id":"bID01cYLr_-Z"}},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","iris = sns.load_dataset(\"iris\")\n","\n","iris_X = iris.iloc[:,0:3]\n","iris_y = iris.iloc[:,[4]]\n","\n","# train_test_split\n","from sklearn.model_selection import train_test_split\n","iris_X_train, iris_X_test, iris_y_train, iris_y_test = train_test_split(iris_X, iris_y, test_size=0.5, shuffle=True, random_state=4)\n","\n","fig, ax = plt.subplots(ncols=3, figsize=(10,5))\n","\n","\n","sns.countplot(x=\"species\", data=iris_y, ax=ax[0])\n","sns.countplot(x=\"species\", data=iris_y_train, ax=ax[1])\n","sns.countplot(x=\"species\", data=iris_y_test, ax=ax[2])\n","plt.show()"],"metadata":{"id":"ASCajihM8Df-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","iris = sns.load_dataset(\"iris\")\n","\n","iris_X = iris.iloc[:,0:3]\n","iris_y = iris.iloc[:,[4]]\n","\n","# train_test_split\n","from sklearn.model_selection import train_test_split\n","iris_X_train, iris_X_test, iris_y_train, iris_y_test = train_test_split(iris_X, iris_y, test_size=0.5, stratify=iris_y, random_state=42)\n","\n","fig, ax = plt.subplots(ncols=3, figsize=(10,5))\n","\n","sns.countplot(x=\"species\", data=iris_y, ax=ax[0])\n","sns.countplot(x=\"species\", data=iris_y_train, ax=ax[1])\n","sns.countplot(x=\"species\", data=iris_y_test, ax=ax[2])\n","plt.show()"],"metadata":{"id":"ja64q8g5cGOo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Repeated holdout\n","\n","The following plots illustrate the issue of increasing the pessimistic bias of a performance estimate if we make the test set too large -- because we withold too many examples for model training such that the model doesn't reach it's capacity -- this assumes that we would fit the model on the whole training set after model evaluation. On the other hand, if we decrease the size of the test set, the estimate of the generalization performance will have a larger variance."],"metadata":{"id":"gDUYdntjzF_L"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from mlxtend.data import iris_data\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","iris_X, iris_y = iris_data()\n","\n","clf_1 = KNeighborsClassifier(n_neighbors=3,\n","                             weights='uniform', \n","                             algorithm='kd_tree', \n","                             leaf_size=30, \n","                             p=2, \n","                             metric='minkowski', \n","                             metric_params=None, \n","                             n_jobs=1)\n","\n","test_sizes=[0.05, 0.2, 0.9]\n","\n","for size in test_sizes:\n","  rng = np.random.RandomState(seed=12345)\n","  seeds = np.arange(10**5)\n","  rng.shuffle(seeds)\n","  seeds = seeds[:50]\n","  pred_2 = []\n","  for i in seeds:\n","      iris_X_train, iris_X_test, iris_y_train, iris_y_test = train_test_split(iris_X, iris_y,\n","                                                          test_size=size, \n","                                                          random_state=i,\n","                                                          stratify=iris_y)\n","      y_pred_i = clf_1.fit(iris_X_train, iris_y_train).predict(iris_X_test)\n","      y_pred_i_acc = np.mean(iris_y_test == y_pred_i)\n","      pred_2.append(y_pred_i_acc)\n","\n","  pred_2 = np.asarray(pred_2)\n","  print()\n","  with plt.style.context(('fivethirtyeight')):\n","      plt.bar(range(0, pred_2.shape[0]), pred_2, color='gray', alpha=0.7)\n","      plt.axhline(pred_2.max(), color='k', linewidth=1, linestyle='--')\n","      plt.axhline(pred_2.min(), color='k', linewidth=1, linestyle='--')\n","      plt.axhspan(pred_2.min(), pred_2.max(), alpha=0.2, color='steelblue')\n","      plt.ylim([0, pred_2.max() + 0.1])\n","      plt.xlabel('Repetition with test_size: {}'.format(size))\n","      plt.ylabel('Accuracy')\n","      plt.ylim([0.5, 1.05])\n","      plt.title('Average: {:.2f}%\\n Variance: {:.2f}'.format(np.mean(pred_2)*100, np.var(pred_2)*100))\n","      plt.tight_layout()\n","      plt.show()"],"metadata":{"id":"T-nSBcj_hr32"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Cross-Validation"],"metadata":{"id":"JZyihTJBzI5z"}},{"cell_type":"markdown","source":["### Setup"],"metadata":{"id":"LZGcuadUlMFG"}},{"cell_type":"code","source":["# Common imports\n","import sklearn\n","import numpy as np"],"metadata":{"id":"Mk9VMCLdlOmz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###  KFold\n","**- sklearn.model_selection.[KFold(n_splits=5, *, shuffle=False, random_state=None)](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)**\n","\n","Provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds (without shuffling by default). Each fold is then used once as a validation while the k - 1 remaining folds form the training set.\n","\n","* n_splits: int, default=5\n","  * Number of folds. Must be at least 2.\n","* shuffle: bool, default=False\n","  * Whether to shuffle the data before splitting into batches."],"metadata":{"id":"TtdbGlY3yp3W"}},{"cell_type":"markdown","source":["**n_splits=5**"],"metadata":{"id":"jlrol3-r5_uH"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import KFold\n","\n","iris = sns.load_dataset(\"iris\")\n","\n","iris_X = iris.iloc[:,0:3]\n","iris_y = iris.iloc[:,[4]]\n","\n","kf = KFold(n_splits=5)\n","\n","fig, ax = plt.subplots(ncols=5, figsize=(15,5))\n","\n","for idx, (train_idx, test_idx)  in enumerate(kf.split(iris_X, iris_y)):\n","  sns.countplot(x=\"species\", data=iris_y.iloc[train_idx], ax=ax[idx])\n","\n","plt.show()"],"metadata":{"id":"DOnf4o1XVS_y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**shuffle=True**"],"metadata":{"id":"nbOtLuhDZj59"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import KFold\n","\n","iris = sns.load_dataset(\"iris\")\n","\n","iris_X = iris.iloc[:,0:3]\n","iris_y = iris.iloc[:,[4]]\n","\n","kf = KFold(shuffle=True)\n","\n","fig, ax = plt.subplots(ncols=5, figsize=(15,5))\n","\n","for idx, (train_idx, test_idx)  in enumerate(kf.split(iris_X, iris_y)):\n","  sns.countplot(x=\"species\", data=iris_y.iloc[train_idx], ax=ax[idx])\n","\n","plt.show()"],"metadata":{"id":"BxD4y52YZCRD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### StratifiedKFold\n","\n","**- sklearn.model_selection.[StratifiedKFold(n_splits=5, *, shuffle=False, random_state=None)](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)**\n","\n","Provides train/test indices to split data in train/test sets.\n","\n","This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."],"metadata":{"id":"DlbATARUThnU"}},{"cell_type":"markdown","source":["*  As discussed in the lecture, it's important to stratify the splits (very crucial for small datasets!)\n"],"metadata":{"id":"vKXMGCplzafy"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import StratifiedKFold\n","\n","iris = sns.load_dataset(\"iris\")\n","\n","iris_X = iris.iloc[:,0:3]\n","iris_y = iris.iloc[:,[4]]\n","\n","sf = StratifiedKFold(shuffle=True)\n","\n","fig, ax = plt.subplots(ncols=5, figsize=(15,5))\n","\n","for idx, (train_idx, test_idx)  in enumerate(sf.split(iris_X, iris_y)):\n","  sns.countplot(x=\"species\", data=iris_y.iloc[train_idx], ax=ax[idx])\n","\n","plt.show()"],"metadata":{"id":"YveJF9Y8yf16"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### cross_val_score"],"metadata":{"id":"nxTzjvRdbT7l"}},{"cell_type":"markdown","source":["**- sklearn.model_selection.[cross_val_score(estimator, X, y=None, *, groups=None, scoring=None, cv=None, n_jobs=None, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', error_score=nan)](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)**\n","\n","**CV**\n","  * default 5-fold\n","  * Determines the cross-validation splitting strategy"],"metadata":{"id":"UqlCVFhVzvRV"}},{"cell_type":"markdown","source":["* visualization 함수"],"metadata":{"id":"nHBtMAPSctQM"}},{"cell_type":"code","source":["%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","def viz_val_bar(param_range, test_acc, name):\n","  idx = np.arange(len(param_range))\n","  plt.figure(figsize=(10,5))\n","  plt.bar(idx, test_acc, width=0.3)\n","  plt.ylabel('Accuracy')\n","  plt.xlabel(name, fontsize=15)\n","  plt.show()"],"metadata":{"id":"O4Uq7PJwctQM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import KFold\n","from sklearn.tree import DecisionTreeClassifier\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","cv_acc = cross_val_score(estimator=DecisionTreeClassifier(random_state=123, max_depth=3),\n","                         X=iris_X,\n","                         y=iris_y,\n","                         cv=KFold(n_splits=10),\n","                         n_jobs=-1)\n","\n","print('Average Accuracy: %.2f%%' % (np.mean(cv_acc)*100))\n","print('Variance: {:.2f}'.format(np.var(cv_acc)*100))\n","viz_val_bar(range(len(cv_acc)),cv_acc,'KFold')"],"metadata":{"id":"NfjUOuRQyivU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.tree import DecisionTreeClassifier\n","\n","cv_acc = cross_val_score(estimator=DecisionTreeClassifier(random_state=123, max_depth=3),\n","                         X=iris_X,\n","                         y=iris_y,\n","                         cv=StratifiedKFold(n_splits=10, random_state=123, shuffle=True),\n","                         n_jobs=-1)\n","\n","print('Average Accuracy: %.2f%%' % (np.mean(cv_acc)*100))\n","print('Variance: {:.2f}'.format(np.var(cv_acc)*100))\n","viz_val_bar(range(len(cv_acc)),cv_acc,'StratifiedKFold')"],"metadata":{"id":"pzspb2Oiyk1H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Validation_curve\n","\n","**- sklearn.model_selection.[validation_curve(estimator, X, y, *, param_name, param_range, groups=None, cv=None, scoring=None, n_jobs=None, pre_dispatch='all', verbose=0, error_score=nan, fit_params=None)](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html) : Returns Scores on training sets and Scores on test set.**\n","\n","Determine training and test scores for varying parameter values.\n","\n","Compute scores for an estimator with different values of a specified parameter. This is similar to grid search with one parameter. However, this will also compute training scores and is merely a utility for plotting the results."],"metadata":{"id":"PKc8dnoi82P0"}},{"cell_type":"markdown","source":["#### California Housing dataset\n","\n","* The target variable is the median house value for California districts, expressed in hundreds of thousands of dollars.\n","* target값을 $100,000 기준으로 반올림하여 6개의 class로 범주화 함"],"metadata":{"id":"C9rSlsgchFVu"}},{"cell_type":"code","source":["from sklearn.datasets import fetch_california_housing\n","import pandas as pd\n","\n","housing = fetch_california_housing()\n","housing_X = housing.data\n","housing_y = np.round(housing.target).astype(int) # make y discrete\n","housing_feature_names = housing.feature_names\n","\n","print('Number of target: ',len(set(housing_y)))\n","\n","# combine features and class data\n","housing_np = np.append(housing_X, housing_y.reshape(-1,1), axis=1)\n","housing_col_names = housing_feature_names + ['class']\n","\n","housing_pd = pd.DataFrame(housing_np, columns=housing_col_names)\n","housing_pd.head(3)"],"metadata":{"id":"vwHuEhaAhFVz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Visualization 함수"],"metadata":{"id":"lDMBl_H6JQ2i"}},{"cell_type":"code","source":["%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","\n","# 수치형 파라미터 시각화 함수\n","def viz_val_curve(param_range, train_mean, train_std, test_mean, test_std, param_name, xscale_log=False):\n","  plt.plot(param_range, train_mean, \n","          color='blue', marker='o', \n","          markersize=5, label='Training accuracy')\n","\n","  plt.fill_between(param_range, train_mean + train_std,\n","                  train_mean - train_std, alpha=0.15,\n","                  color='blue')\n","\n","  plt.plot(param_range, test_mean, \n","          color='green', linestyle='--', \n","          marker='s', markersize=5, \n","          label='Validation accuracy')\n","\n","  plt.fill_between(param_range, \n","                  test_mean + test_std,\n","                  test_mean - test_std, \n","                  alpha=0.15, color='green')\n","\n","\n","  plt.grid()\n","  plt.legend(loc='lower right')\n","  if xscale_log:\n","    plt.xscale('log')\n","  plt.xlabel(param_name)\n","  plt.ylabel('Accuracy')\n","  plt.ylim([np.min(test_mean)*0.8, np.max(train_mean)*1.2])\n","  plt.tight_layout()\n","  plt.show()\n","\n","# 범주형 파라미터 시각화 함수\n","def viz_val_bar(param_range, train_mean, train_std, test_mean, test_std, param_name):\n","  idx = np.arange(len(param_range))\n","  plt.bar(idx, test_mean, width=0.3)\n","  plt.xlabel(param_name)\n","  plt.ylabel('Accuracy')\n","  plt.ylim([np.min(test_mean)*0.9, np.max(test_mean)*1.1])\n","  plt.xticks(idx, param_range, fontsize=15)\n","  plt.show()"],"metadata":{"id":"-j85_BOxJTNY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**cv**"],"metadata":{"id":"fQJ99QkNhRBg"}},{"cell_type":"code","source":["from sklearn.model_selection import validation_curve\n","\n","param_range= [i for i in range(1,10)]\n","param_name='max_depth'\n","\n","from sklearn.tree import DecisionTreeClassifier\n","\n","dt_model_house_m = DecisionTreeClassifier(random_state=42)\n","\n","train_scores, test_scores = validation_curve(\n","                estimator=dt_model_house_m, \n","                X=housing_X, \n","                y=housing_y, \n","                param_name=param_name, \n","                param_range=param_range,\n","                scoring='accuracy',\n","                cv=10)\n","\n","train_mean = np.mean(train_scores, axis=1)\n","train_std = np.std(train_scores, axis=1)\n","test_mean = np.mean(test_scores, axis=1)\n","test_std = np.std(test_scores, axis=1)\n","\n","viz_val_curve(param_range, train_mean, train_std, test_mean, test_std, param_name)"],"metadata":{"id":"8Q0nT0uYxl-4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import validation_curve\n","\n","param_range= [i for i in range(1,10)]\n","param_name='max_depth'\n","\n","from sklearn.tree import DecisionTreeClassifier\n","\n","dt_model_house_m = DecisionTreeClassifier(random_state=42)\n","\n","train_scores, test_scores = validation_curve(\n","                estimator=dt_model_house_m, \n","                X=housing_X, \n","                y=housing_y, \n","                param_name=param_name, \n","                param_range=param_range,\n","                scoring='accuracy',\n","                cv=StratifiedKFold(n_splits=10, random_state=123, shuffle=True))\n","\n","train_mean = np.mean(train_scores, axis=1)\n","train_std = np.std(train_scores, axis=1)\n","test_mean = np.mean(test_scores, axis=1)\n","test_std = np.std(test_scores, axis=1)\n","\n","viz_val_curve(param_range, train_mean, train_std, test_mean, test_std, param_name)"],"metadata":{"id":"p5q3i1FGhd6S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluation Metrics"],"metadata":{"id":"h-fADImnzZMy"}},{"cell_type":"markdown","source":["### Loading the Breast Cancer Wisconsin dataset\n","\n","\n","*   In the Breast Cancer Wisconsin dataset, the firt column in this dataset stores the unique ID numbers of patients\n","*   The second column stores the corresponding cancer diagnoses (M = malignant, B = benign)\n","*   Columns 3-32 contain features that were extracted from digitized images of the nuclei of the cancer cells, which can be used to build a model to predict whether a tumor is benign or malignant.\n","*   The Breast Cancer Wisconsin dataset has been deposited in the UCI Machine Learning Repository, and more detailed information about this dataset can be found at https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)."],"metadata":{"id":"fNlGvIRN49IN"}},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv('https://archive.ics.uci.edu/ml/'\n","                 'machine-learning-databases'\n","                 '/breast-cancer-wisconsin/wdbc.data', header=None)\n","\n","print('shape: ',df.shape)\n","df.head()"],"metadata":{"id":"OXQZ_ZcGb7Wb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*   First, we are converting the class labels from a string format into integers"],"metadata":{"id":"2X--OOQ75Rl9"}},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","\n","X = df.loc[:, 2:].values\n","y = df.loc[:, 1].values\n","le = LabelEncoder()\n","y = le.fit_transform(y)\n","le.classes_"],"metadata":{"id":"jHq7JD83bzTE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*   Here, class \"M\" (malignant cancer) will be converted to class 1, and \"B\" will be converted into class 0 (the order the class labels are mapped depends on the alphabetical order of the string labels)"],"metadata":{"id":"oXLMYo5H5YjN"}},{"cell_type":"code","source":["le.transform(['M', 'B'])"],"metadata":{"id":"IWtCiPoVcnks"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*   Next, we split the data into 80% training data and 20% test data, using a stratified split\n"],"metadata":{"id":"Sk_rAyyb5gQ-"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = \\\n","    train_test_split(X, y, \n","                     test_size=0.20,\n","                     stratify=y,                     \n","                     random_state=1)"],"metadata":{"id":"iWvwnw_J5f-g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Confusion Matrix\n","\n","\n"],"metadata":{"id":"JgJ6vVZQb7oL"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.pipeline import make_pipeline\n","from sklearn.metrics import confusion_matrix\n","\n","pipe_knn = make_pipeline(StandardScaler(),\n","                         KNeighborsClassifier(n_neighbors=5))\n","\n","pipe_knn.fit(X_train, y_train)\n","\n","y_pred = pipe_knn.predict(X_test)\n","\n","confmat = confusion_matrix(y_test, y_pred, labels=[1,0])\n","confmat"],"metadata":{"id":"9A3R7C7I53kJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[1,0]).ravel()\n","(tn, fp, fn, tp)"],"metadata":{"id":"HSvXzER3CXk-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Multiclass to Binary"],"metadata":{"id":"CpDytvxDB1a4"}},{"cell_type":"code","source":["y_target =    [1, 1, 1, 0, 0, 2, 0, 3]\n","y_predicted = [1, 0, 1, 0, 0, 2, 1, 3]\n","\n","cm1 = confusion_matrix(y_target,y_predicted)\n","print(cm1)"],"metadata":{"id":"YVkj6w6P7R1t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Visualization"],"metadata":{"id":"VeQKhVY7Ark3"}},{"cell_type":"code","source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","confmat_display = ConfusionMatrixDisplay(confmat).plot()\n","cm1_display = ConfusionMatrixDisplay(cm1, display_labels=[3,2,1,0]).plot()"],"metadata":{"id":"e5mnQWkP_4gd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###More examples\n","\n","*   https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html?highlight=confusion%20matrix#sklearn.metrics.confusion_matrix\n","*   https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix\n","*   http://rasbt.github.io/mlxtend/user_guide/evaluate/confusion_matrix/\n","*   http://rasbt.github.io/mlxtend/user_guide/plotting/plot_confusion_matrix/"],"metadata":{"id":"soPpSTk4oD1c"}},{"cell_type":"markdown","source":["### Precison, Recall, F1 Score and Balanced accuracy"],"metadata":{"id":"Ekhb22v18UzW"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.pipeline import make_pipeline\n","from sklearn.metrics import confusion_matrix\n","\n","pipe_knn = make_pipeline(StandardScaler(),\n","                         KNeighborsClassifier(n_neighbors=5))\n","\n","pipe_knn.fit(X_train, y_train)\n","\n","y_pred = pipe_knn.predict(X_test)\n","\n","confmat = confusion_matrix(y_test, y_pred, labels=[1,0])\n","\n","print(confmat)"],"metadata":{"id":"rb1cRzFFkCCf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, \\\n","                            recall_score, f1_score, matthews_corrcoef, balanced_accuracy_score\n","\n","\n","print('Accuracy: %.3f' % accuracy_score(y_true=y_test, y_pred=y_pred))\n","print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\n","print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\n","print('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))\n","print('MCC: %.3f' % matthews_corrcoef(y_true=y_test, y_pred=y_pred))\n","print('Balanced accuracy: %.3f' % balanced_accuracy_score(y_true=y_test, y_pred=y_pred))"],"metadata":{"id":"Yf2-Vy-xkO7E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Using those Metrics in GridSearch"],"metadata":{"id":"heVKcW3wkWAe"}},{"cell_type":"markdown","source":["**- sklearn.model_selection.[GridSearchCV(estimator, param_grid, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) : Exhaustive search over specified parameter values for an estimator.**"],"metadata":{"id":"mKsTb86pmS5G"}},{"cell_type":"markdown","source":["**[scoring 종류](https://runebook.dev/ko/docs/scikit_learn/modules/model_evaluation)**"],"metadata":{"id":"mdj66tVcsNC7"}},{"cell_type":"markdown","source":["* F1 score"],"metadata":{"id":"1StdwquKftWL"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","\n","param_range_n_neighbors = [3, 5, 7, 9, 15, 21, 31]\n","param_range_weights = ['uniform','distance']\n","\n","pipe_knn = make_pipeline(StandardScaler(),\n","                         KNeighborsClassifier())\n","\n","param_grid = [{'kneighborsclassifier__n_neighbors': param_range_n_neighbors, 'kneighborsclassifier__weights': param_range_weights}]\n","\n","gs = GridSearchCV(estimator=pipe_knn,\n","                  param_grid=param_grid,\n","                  scoring='f1',\n","                  cv=10,\n","                  n_jobs=-1)\n","\n","\n","gs = gs.fit(X_train, y_train)\n","print(gs.best_score_)\n","print(gs.best_params_)"],"metadata":{"id":"sph9l6vykd21"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* average_precision"],"metadata":{"id":"JvHUIwzvfvqy"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","\n","param_range = [3, 5, 7, 9, 15, 21, 31]\n","param_range_weights = ['uniform','distance']\n","\n","pipe_knn = make_pipeline(StandardScaler(),\n","                         KNeighborsClassifier())\n","\n","param_grid = [{'kneighborsclassifier__n_neighbors': param_range, 'kneighborsclassifier__weights': param_range_weights}]\n","\n","\n","gs = GridSearchCV(estimator=pipe_knn,\n","                  param_grid=param_grid,\n","                  scoring='average_precision',\n","                  cv=10,\n","                  n_jobs=-1)\n","\n","\n","gs = gs.fit(X_train, y_train)\n","print(gs.best_score_)\n","print(gs.best_params_)"],"metadata":{"id":"2DmSbBHtks5y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* F1_score, average='micro'"],"metadata":{"id":"Tz_Q_sW2f91y"}},{"cell_type":"code","source":["from sklearn.metrics import make_scorer\n","from mlxtend.data import iris_data\n","\n","X_iris, y_iris = iris_data()\n","\n","# for multiclass:\n","#scorer = make_scorer(f1_score, average='macro')\n","scorer = make_scorer(f1_score, average='micro')\n","\n","from sklearn.model_selection import GridSearchCV\n","\n","param_range = [3, 5, 7, 9, 15, 21, 31]\n","param_range_weights = ['uniform','distance']\n","\n","pipe_knn = make_pipeline(StandardScaler(),\n","                         KNeighborsClassifier())\n","\n","param_grid = [{'kneighborsclassifier__n_neighbors': param_range, 'kneighborsclassifier__weights': param_range_weights}]\n","param_range_weights = ['uniform','distance']\n","\n","gs = GridSearchCV(estimator=pipe_knn,\n","                  param_grid=param_grid,\n","                  scoring=scorer,\n","                  cv=10,\n","                  n_jobs=-1)\n","\n","\n","gs = gs.fit(X_iris, y_iris)\n","print(gs.best_score_)\n","print(gs.best_params_)"],"metadata":{"id":"A37O8XrNklYX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Receiver Operating Characteristic curve(ROC curve)"],"metadata":{"id":"h_HOxWjloX-i"}},{"cell_type":"markdown","source":["#### Load the Breast Cancer Wisconsin dataset"],"metadata":{"id":"UCvQ5hl6jTtD"}},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv('https://archive.ics.uci.edu/ml/'\n","                 'machine-learning-databases'\n","                 '/breast-cancer-wisconsin/wdbc.data', header=None)\n","\n","from sklearn.preprocessing import LabelEncoder\n","\n","X = df.loc[:, 2:].values\n","y = df.loc[:, 1].values\n","le = LabelEncoder()\n","y = le.fit_transform(y)\n","le.classes_\n","\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = \\\n","    train_test_split(X, y, \n","                     test_size=0.20,\n","                     stratify=y,                     \n","                     random_state=1)"],"metadata":{"id":"7VUczOqjjTtD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn import metrics\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.pipeline import make_pipeline\n","\n","# smaller training set to make the curve more interesting\n","X_train2 = X_train[:, [4, 14]]\n","X_test2 = X_test[:, [4, 14]]\n","\n","pipe_knn = make_pipeline(StandardScaler(),\n","                         KNeighborsClassifier())\n","pipe_knn.fit(X_train2, y_train)\n","\n","## train roc curve\n","y_preds_train = pipe_knn.predict_proba(X_train2)\n","fpr, tpr, thresholds = metrics.roc_curve(y_train, y_preds_train[:,1])\n","roc_auc = metrics.auc(fpr, tpr)\n","display_train = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n","                                  estimator_name='Train')\n","# viz\n","display_train.plot(color='red')\n","plt.plot([0, 1],\n","         [0, 1],\n","         linestyle='--',\n","         color='black')\n","plt.plot([0, 0, 1],\n","         [0, 1, 1],\n","         linestyle=':',\n","         color='black')\n","\n","## test roc curve\n","y_preds_test = pipe_knn.predict_proba(X_test2)\n","fpr, tpr, thresholds = metrics.roc_curve(y_test, y_preds_test[:,1])\n","roc_auc = metrics.auc(fpr, tpr)\n","display_test = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n","                                  estimator_name='Test')\n","# viz\n","display_test.plot()\n","plt.plot([0, 1],\n","         [0, 1],\n","         linestyle='--',\n","         color='black')\n","plt.plot([0, 0, 1],\n","         [0, 1, 1],\n","         linestyle=':',\n","         color='black')\n","plt.show()"],"metadata":{"id":"cMrTUE-NjAUq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise"],"metadata":{"id":"yX2Xn3OsNp44"}},{"cell_type":"markdown","source":["## 1번 문제 (Confidence interval and Resamping)\n","iris 데이터를 아래 모델들을 사용하여 분류하고, cross validation curve함수를 사용하여 Test set의 accuracy가 가장 높은 파라미터 조합을 도출하시오.\n","  * GaussianNB\n","  * kNeighborsClassifier\n","\n","* Iris dataset\n","\n","```python\n","from sklearn.datasets import load_iris\n","iris = load_iris() \n","```"],"metadata":{"id":"NCmqu3MKg1Wo"}},{"cell_type":"markdown","source":["## 1번 문제 답안"],"metadata":{"id":"Ldaxo9PbMtB7"}},{"cell_type":"code","source":[""],"metadata":{"id":"Pk-SHIK6Pc_5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2번 문제\n","\n","Breast cancer 데이터를 사용하여 아래 Evaluation Metrics를 표현하시오.\n","\n","* KNeighborsClassifier를 이용하여 데이터를 분류하고 confusion metrix로 결과를 도출하시오.\n","* DecisionTreeClassifier를 이용하여 데이터를 분류하고 Precision, Recall, F1 Score, Balanced_accuracy를 도출하시오.\n","\n","\n","\n","```python\n","from sklearn import datasets\n","import pandas as pd\n","\n","breast = datasets.load_breast_cancer()\n","breast_X = breast[\"data\"]\n","breast_y = breast[\"target\"]\n","breast_feature_name = breast.feature_names\n","\n","pd.DataFrame(breast_X, columns=breast_feature_name).head(5)\n","```\n","\n"],"metadata":{"id":"33d20OHeMvSj"}},{"cell_type":"markdown","source":["## 2번 문제 답안"],"metadata":{"id":"-R8CL1qSPaba"}},{"cell_type":"code","source":[""],"metadata":{"id":"750AYmHJPcsY"},"execution_count":null,"outputs":[]}]}